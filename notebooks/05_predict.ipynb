{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow.lite as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruifspinto/.local/share/virtualenvs/mlzoomcamp2024-final-project-2NtNqQ0o/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv(os.path.join('..','data','processed','data_for_model.csv'))\n",
    "\n",
    "# load the model\n",
    "best_model= tf.keras.models.load_model(os.path.join('..','models','best_model.keras'))\n",
    "\n",
    "# load feature_info, ct and label_encoder\n",
    "feature_info = np.load(os.path.join('..','models','feature_info.npy'), allow_pickle=True).item()\n",
    "ct = joblib.load(os.path.join('..', 'models', 'column_transformer.pkl'))\n",
    "label_encoder = joblib.load(os.path.join('..', 'models', 'label_encoder.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Keras to TF-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxa4bexdd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxa4bexdd/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpxa4bexdd'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 172), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140181091725232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140180558372480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140181627041424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140180558647840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140180558656640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140180558646608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1737665315.782806    5140 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1737665315.782842    5140 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-01-23 20:48:35.782980: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpxa4bexdd\n",
      "2025-01-23 20:48:35.783451: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-01-23 20:48:35.783461: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpxa4bexdd\n",
      "2025-01-23 20:48:35.786777: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-01-23 20:48:35.805189: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpxa4bexdd\n",
      "2025-01-23 20:48:35.811191: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 28213 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(os.path.join('..','models','best_model.tflite'), 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the function to predict football match results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_match_outcome(data, home_team, away_team, date_str, ct, best_model_dense, label_encoder, feature_info):\n",
    "    \"\"\"\n",
    "    Predict the outcome of a football match based on historical data and features.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The historical match data.\n",
    "        home_team (str): Home team's name.\n",
    "        away_team (str): Away team's name.\n",
    "        date_str (str): Date of the match in 'YYYY-MM-DD' format.\n",
    "        ct (ColumnTransformer): Preprocessing pipeline from training.\n",
    "        best_model_dense (keras.Model): Trained neural network model.\n",
    "        label_encoder (LabelEncoder): Encoder for target classes.\n",
    "        feature_info (dict): Dictionary containing `categorical_features` and `numerical_features`.\n",
    "\n",
    "    Returns:\n",
    "        dict: Predicted probabilities and the predicted result.\n",
    "    \"\"\"\n",
    "    # Extract feature info\n",
    "    categorical_features = feature_info['categorical_features']\n",
    "    numerical_features = feature_info['numerical_features']\n",
    "\n",
    "    # Step 1: Define home and away team columns\n",
    "    team_columns_home = [col for col in data.columns if col.startswith('home_')]\n",
    "    team_columns_away = [col for col in data.columns if col.startswith('away_')]\n",
    "\n",
    "    # Step 2: Get the latest features for both teams\n",
    "    team_data_home = data[data['home_team'] == home_team].tail(1)[team_columns_home]\n",
    "    team_data_away = data[data['away_team'] == away_team].tail(1)[team_columns_away]\n",
    "\n",
    "    # Combine features into a single row\n",
    "    combined_features = pd.concat([team_data_home.reset_index(drop=True), \n",
    "                                    team_data_away.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Step 3: Add date-related features\n",
    "    date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "    day_of_week = date.dayofweek\n",
    "    month = date.month\n",
    "    day_of_week_sin = np.sin(2 * np.pi * day_of_week / 7.0)\n",
    "    day_of_week_cos = np.cos(2 * np.pi * day_of_week / 7.0)\n",
    "    month_sin = np.sin(2 * np.pi * month / 12.0)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12.0)\n",
    "\n",
    "    date_features = pd.DataFrame([{\n",
    "        'day_of_week': day_of_week,\n",
    "        'month': month,\n",
    "        'day_of_week_sin': day_of_week_sin,\n",
    "        'day_of_week_cos': day_of_week_cos,\n",
    "        'month_sin': month_sin,\n",
    "        'month_cos': month_cos\n",
    "    }])\n",
    "\n",
    "    # Combine all features\n",
    "    final_features = pd.concat([combined_features, date_features], axis=1)\n",
    "\n",
    "    # Step 4: Add missing features\n",
    "    # Add missing categorical features with a default \"missing\" value\n",
    "    for feature in categorical_features:\n",
    "        if feature not in final_features.columns:\n",
    "            final_features[feature] = \"missing\"\n",
    "\n",
    "    # Add missing numerical features with a default value of 0\n",
    "    for feature in numerical_features:\n",
    "        if feature not in final_features.columns:\n",
    "            final_features[feature] = 0\n",
    "\n",
    "    # Reorder the columns to match the training feature order\n",
    "    final_features = final_features[categorical_features + numerical_features]\n",
    "\n",
    "    # Debug: Check final features before preprocessing\n",
    "    #print(\"Final Features Before Preprocessing:\\n\", final_features)\n",
    "\n",
    "    # Step 5: Preprocess the features\n",
    "    X_new_processed = ct.transform(final_features)\n",
    "\n",
    "    # Debug: Check processed features\n",
    "    #print(\"Processed Features:\\n\", X_new_processed)\n",
    "\n",
    "    # Step 6: Predict probabilities\n",
    "    y_pred = best_model_dense.predict(X_new_processed)\n",
    "\n",
    "    # Map probabilities to outcomes\n",
    "    probabilities = {\n",
    "        'Home Win': y_pred[0][label_encoder.transform(['H'])[0]],\n",
    "        'Draw': y_pred[0][label_encoder.transform(['D'])[0]],\n",
    "        'Away Win': y_pred[0][label_encoder.transform(['A'])[0]]\n",
    "    }\n",
    "\n",
    "    # Predicted class\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    predicted_label = label_encoder.inverse_transform(y_pred_classes)[0]\n",
    "\n",
    "    return {\n",
    "        'probabilities': probabilities,\n",
    "        'predicted_result': predicted_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# load tf-lite model\n",
    "\n",
    "\n",
    "interpreter = tflite.Interpreter(model_path=os.path.join('..','models','best_model.tflite'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'probabilities': {'Home Win': np.float32(0.2672754),\n",
       "  'Draw': np.float32(0.49714017),\n",
       "  'Away Win': np.float32(0.23558447)},\n",
       " 'predicted_result': 'D'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_team = 'everton'\n",
    "away_team = 'wolves'\n",
    "date_str = '2021-05-19'\n",
    "\n",
    "# Predict\n",
    "prediction = predict_match_outcome(\n",
    "    data=data,\n",
    "    home_team=home_team,\n",
    "    away_team=away_team,\n",
    "    date_str=date_str,\n",
    "    ct=ct,  # Preprocessing pipeline\n",
    "    best_model_dense=best_model,  # Trained model\n",
    "    label_encoder=label_encoder,  # Target encoder\n",
    "    feature_info=feature_info  # Training feature information\n",
    ")\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' url = \"http://0.0.0.0:9696/predict\"\\npayload = {\\n    \"home_team\": \"everton\",\\n    \"away_team\": \"wolves\",\\n    \"date\": \"2021-05-19\"\\n}\\n\\nresponse = requests.post(url, json=payload)\\nprint(response.json()) '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://0.0.0.0:9696/predict\"\n",
    "payload = {\n",
    "    \"home_team\": \"everton\",\n",
    "    \"away_team\": \"wolves\",\n",
    "    \"date\": \"2021-05-19\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Match_Result': 'Home_Win', 'Prob_Away_Win': 0.008116791024804115, 'Prob_Draw': 0.1457294374704361, 'Prob_Home_Win': 0.8461537957191467}\n",
      "{'Match_Result': 'Away_Win', 'Prob_Away_Win': 0.5223577618598938, 'Prob_Draw': 0.32547494769096375, 'Prob_Home_Win': 0.15216724574565887}\n",
      "{'Match_Result': 'Home_Win', 'Prob_Away_Win': 0.15303798019886017, 'Prob_Draw': 0.3209175765514374, 'Prob_Home_Win': 0.5260443687438965}\n",
      "{'Match_Result': 'Home_Win', 'Prob_Away_Win': 0.008386810310184956, 'Prob_Draw': 0.13073571026325226, 'Prob_Home_Win': 0.8608774542808533}\n"
     ]
    }
   ],
   "source": [
    "#test multiple matches\n",
    "for match in [\n",
    "    {\"home_team\": \"arsenal\", \"away_team\": \"brentford\", \"date\": \"2021-08-13\"},\n",
    "    {\"home_team\": \"liverpool\", \"away_team\": \"chelsea\", \"date\": \"2021-08-13\"},\n",
    "    {\"home_team\": \"brentford\", \"away_team\": \"arsenal\", \"date\": \"2021-08-14\"},\n",
    "    {\"home_team\": \"arsenal\", \"away_team\": \"brentford\", \"date\": \"2021-08-14\"},\n",
    "]:\n",
    "    print(requests.post(url, json=match).json())  # test the API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Match_Result\": \"Home_Win\",\n",
      "  \"Prob_Away_Win\": 0.0622507706284523,\n",
      "  \"Prob_Draw\": 0.39442455768585205,\n",
      "  \"Prob_Home_Win\": 0.5433247685432434\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://127.0.0.1:9696/predict \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"home_team\": \"arsenal\", \"away_team\": \"liverpool\", \"date\": \"2024-12-16\"}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlzoomcamp2024-final-project-2NtNqQ0o",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
